"""
 This module include a list of method & object of Utility for
 supporting the Article Fact Check
"""

import google.generativeai as genai
import os
from dotenv import load_dotenv
import requests
import time
from typing import List, Optional
from bs4 import BeautifulSoup  
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm  # Optional: For progress visualization
from ViFinanceCrawLib.article_database.ArticleScraper import ArticleScraper
from ViFinanceCrawLib.article_database.SearchEngine import SearchEngine
from ViFinanceCrawLib.article_database.SearchEngine import SearchEngine
from ViFinanceCrawLib.article_database.TitleCompleter import TitleCompleter
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

import json
import re
class ArticleFactCheckUtility():

    def __init__(self, model_name='gemini-2.0-flash-lite'):
        load_dotenv()
        genai.configure(api_key=os.getenv("API_KEY"))
        self.model_name = model_name
        self.model = genai.GenerativeModel(model_name)
        self.tc = TitleCompleter()

        self.search_engine  =  SearchEngine(os.getenv("SEARCH_API_KEY"), os.getenv("SEARCH_ENGINE_ID"))
        self.article_scraper = ArticleScraper()
        return
        
    def generate_search_queries(self, statement):
        prompt = f"""Identify keywords and concepts from the following statement. Using this claim, generate a neutral, thought-provoking question that encourages discussion without assuming a particular stance. The question must be between 10 and 50 words long. Return only the generated question with no other surrounding text, and you MUST write it in Vietnamese.

        Statement: {statement}
        """
        response = self.model.generate_content(prompt)
        claims = response.text.split('\n')
        claims = [claim.strip() for claim in claims if claim.strip()]  # Clean up
        return claims
    
    def fact_check_article_using_query(self, article_text):
        """
        Fact-check an article by generating neutral, thought-provoking search queries
        based on its content.

        Parameters:
        - article_text (str): The full text of the article to fact-check.

        Returns:
        - List of generated Vietnamese search queries.
        """
        # Step 1: Create a prompt to extract key claims/questions from article
        prompt = f"""Identify keywords and concepts from the following article. 
                    Using this article, generate neutral, thought-provoking questions that encourage 
                    discussion without assuming a particular stance. Each question must be between 
                    10 and 50 words long. 

                    You must return the questions in the following format:

                    Query 1: <question in Vietnamese>
                    Query 2: <question in Vietnamese>
                    ...

                    Do not include any other text.

                    Article: {article_text}
                    """
        # Step 2: Get response from model
        response = self.model.generate_content(prompt)

        # Step 3: Process and clean the output
        raw_output = response.text.split('\n')
        search_queries = []
        for line in raw_output:
            line = line.strip()
            if line.startswith("Query"):
                # Extract after 'Query X:'
                query_text = line.split(":", 1)[1].strip()
                if query_text:
                    search_queries.append(query_text)

        return search_queries
    
    def search_web(self, query, num_results=5):
        tc = self.tc
        searchEngine = self.search_engine
        articles = searchEngine.search(query, num=num_results)
        article_scraper = self.article_scraper
        valid_articles = []
        for article in articles:
            try:
                if not article.get("link"):
                    continue  # Skip articles with no links

                original_title = article["title"]
                # Scrape article content
                article_data = article_scraper.scrape_article(article["link"])

                if (
                    not article_data["main_text"]
                    or article_data["main_text"] == "No content available"
                    or article_data["author"] == "Unknown"
                ):
                    continue  # Skip invalid articles

                # ‚úÖ Complete title AFTER filtering
                article_data["title"] = tc.complete_title(original_title=original_title, article=article)
                valid_articles.append(article_data)

            except Exception as e:
                print(f"‚ùå Error processing article {article['link']}: {e}")
        return valid_articles
    
    def search_web_fast(self, query, num_results=5):
        """
        Main method: Search articles and scrape them in parallel.
        """
        articles = self.search_engine.search(query, num=num_results)
        valid_articles = self.scrape_articles_parallel(articles, batch_size=num_results)
        return valid_articles

    def scrape_articles_parallel(self, articles, batch_size=5):
        """
        Helper method: Scrape articles in parallel.
        Scrape articles concurrently in batches.
        """
        valid_articles = []

        with ThreadPoolExecutor(max_workers=batch_size) as executor:
            futures = [executor.submit(self.process_single_article, article) for article in articles]

            for future in tqdm(as_completed(futures), total=len(futures), desc="Scraping Articles"):
                result = future.result()
                if result:
                    valid_articles.append(result)

        return valid_articles

    def process_single_article(self, article):
        """
        Helper method: Process a single article.
        Process a single article: Scrape, validate, and format.
        """
        try:
            if not article.get("link"):
                return None  # Skip articles with no links

            original_title = article["title"]

            # Scrape article content
            article_data = self.article_scraper.scrape_article(article["link"])

            # Filter invalid articles
            if (
                not article_data["main_text"]
                or article_data["main_text"] == "No content available"
                or article_data["author"] == "Unknown"
            ):
                return None

            # Complete title after filtering
            article_data["title"] = self.tc.complete_title(original_title=original_title, article=article)
            return article_data

        except Exception as e:
            print(f"‚ùå Error processing article {article.get('link')}: {e}")
            return None

    def analyze_evidence(self, statement, evidence):
        # Cretability metric - https://rusi-ns.ca/a-system-to-judge-information-reliability/
        """
        Analyzes evidence to determine if it supports, contradicts, or is neutral to the statement.
        """
        prompt = f"""B·∫°n l√† m·ªôt tr·ª£ l√Ω ph√¢n t√≠ch v√† ƒë√°nh gi√° t√≠nh x√°c th·ª±c c·ªßa th√¥ng tin. D∆∞·ªõi ƒë√¢y l√† m·ªôt m·ªánh ƒë·ªÅ & th√¥ng tin v√† m·ªôt t·∫≠p h·ª£p b·∫±ng ch·ª©ng. H√£y ƒë√°nh gi√° m·ª©c ƒë·ªô m√† b·∫±ng ch·ª©ng h·ªó tr·ª£, m√¢u thu·∫´n ho·∫∑c trung l·∫≠p ƒë·ªëi v·ªõi th√¥ng tin, b·∫±ng c√°ch xem x√©t:
                ‚Ä¢ M·ªëi quan h·ªá logic gi·ªØa tuy√™n b·ªë v√† b·∫±ng ch·ª©ng.
                ‚Ä¢ ƒê·ªô m·∫°nh c·ªßa b·∫±ng ch·ª©ng, bao g·ªìm ngu·ªìn g·ªëc, t√≠nh ch√≠nh x√°c v√† m·ª©c ƒë·ªô li√™n quan.
                ‚Ä¢ B·ªëi c·∫£nh v√† gi·∫£ ƒë·ªãnh ti·ªÅm ·∫©n c√≥ th·ªÉ ·∫£nh h∆∞·ªüng ƒë·∫øn di·ªÖn gi·∫£i b·∫±ng ch·ª©ng.

            ### **H·ªá th·ªëng ƒë√°nh gi√° ƒë·ªô tin c·∫≠y**  
            H√£y ƒë√°nh gi√° m·ª©c ƒë·ªô tin c·∫≠y c·ªßa t·ª´ng ngu·ªìn v√† t·ª´ng th√¥ng tin b·∫±ng h·ªá th·ªëng NATO:  

            - **ƒê√°nh gi√° ƒë·ªô tin c·∫≠y c·ªßa ngu·ªìn** (Ch·ªØ c√°i):  
            - **A**: Ho√†n to√†n ƒë√°ng tin c·∫≠y  
            - **B**: ƒê√°ng tin c·∫≠y  
            - **C**: Kh√° ƒë√°ng tin c·∫≠y  
            - **D**: Kh√¥ng ƒë√°ng tin c·∫≠y  
            - **E**: Kh√¥ng th·ªÉ ƒë√°nh gi√°  

            - **ƒê√°nh gi√° ƒë·ªô ch√≠nh x√°c c·ªßa th√¥ng tin** (Ch·ªØ s·ªë):  
            - **1**: ƒê√£ ƒë∆∞·ª£c x√°c minh  
            - **2**: C√≥ kh·∫£ nƒÉng ƒë√∫ng  
            - **3**: C√≥ th·ªÉ ƒë√∫ng  
            - **4**: Kh√¥ng ch·∫Øc ch·∫Øn  
            - **5**: Kh√¥ng th·ªÉ ƒë√°nh gi√°  

            K·∫øt qu·∫£ ƒë√°nh gi√° s·∫Ω ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng **A1, B2, C3, v.v.**, trong ƒë√≥:  
            - **A1** l√† th√¥ng tin ƒë√°ng tin c·∫≠y nh·∫•t, c√≥ ngu·ªìn m·∫°nh v√† ƒë√£ ƒë∆∞·ª£c x√°c minh.  
            - **E5** l√† th√¥ng tin ƒë√°ng tin c·∫≠y k√©m nh·∫•t, c√≥ ngu·ªìn y·∫øu v√† kh√¥ng th·ªÉ ƒë√°nh gi√°.  

            ### ** DANH S√ÅCH √ù nghƒ©a c·ªßa c√°c t·ªï h·ª£p ƒë√°nh gi√° [PH·∫¢I D√ôNG ƒê√öNG CH√çNH X√ÅC C√ö PH√ÅP NH∆Ø G·ªêC]**  
            A1: Ho√†n to√†n ƒë√°ng tin c·∫≠y & ƒê√£ ƒë∆∞·ª£c x√°c minh  
            A2: Ho√†n to√†n ƒë√°ng tin c·∫≠y & C√≥ kh·∫£ nƒÉng ƒë√∫ng  
            A3: Ho√†n to√†n ƒë√°ng tin c·∫≠y & C√≥ th·ªÉ ƒë√∫ng  
            A4: Ho√†n to√†n ƒë√°ng tin c·∫≠y & Kh√¥ng ch·∫Øc ch·∫Øn  
            A5: Ho√†n to√†n ƒë√°ng tin c·∫≠y & Kh√¥ng th·ªÉ ƒë√°nh gi√°  
            B1: ƒê√°ng tin c·∫≠y & ƒê√£ ƒë∆∞·ª£c x√°c minh  
            B2: ƒê√°ng tin c·∫≠y & C√≥ kh·∫£ nƒÉng ƒë√∫ng  
            B3: ƒê√°ng tin c·∫≠y & C√≥ th·ªÉ ƒë√∫ng  
            B4: ƒê√°ng tin c·∫≠y & Kh√¥ng ch·∫Øc ch·∫Øn  
            B5: ƒê√°ng tin c·∫≠y & Kh√¥ng th·ªÉ ƒë√°nh gi√°  
            C1: Kh√° ƒë√°ng tin c·∫≠y & ƒê√£ ƒë∆∞·ª£c x√°c minh  
            C2: Kh√° ƒë√°ng tin c·∫≠y & C√≥ kh·∫£ nƒÉng ƒë√∫ng  
            C3: Kh√° ƒë√°ng tin c·∫≠y & C√≥ th·ªÉ ƒë√∫ng  
            C4: Kh√° ƒë√°ng tin c·∫≠y & Kh√¥ng ch·∫Øc ch·∫Øn  
            C5: Kh√° ƒë√°ng tin c·∫≠y & Kh√¥ng th·ªÉ ƒë√°nh gi√°  
            D1: Kh√¥ng ƒë√°ng tin c·∫≠y & ƒê√£ ƒë∆∞·ª£c x√°c minh  
            D2: Kh√¥ng ƒë√°ng tin c·∫≠y & C√≥ kh·∫£ nƒÉng ƒë√∫ng  
            D3: Kh√¥ng ƒë√°ng tin c·∫≠y & C√≥ th·ªÉ ƒë√∫ng  
            D4: Kh√¥ng ƒë√°ng tin c·∫≠y & Kh√¥ng ch·∫Øc ch·∫Øn  
            D5: Kh√¥ng ƒë√°ng tin c·∫≠y & Kh√¥ng th·ªÉ ƒë√°nh gi√°  
            E1: Kh√¥ng th·ªÉ ƒë√°nh gi√° & ƒê√£ ƒë∆∞·ª£c x√°c minh  
            E2: Kh√¥ng th·ªÉ ƒë√°nh gi√° & C√≥ kh·∫£ nƒÉng ƒë√∫ng  
            E3: Kh√¥ng th·ªÉ ƒë√°nh gi√° & C√≥ th·ªÉ ƒë√∫ng  
            E4: Kh√¥ng th·ªÉ ƒë√°nh gi√° & Kh√¥ng ch·∫Øc ch·∫Øn  
            E5: Kh√¥ng th·ªÉ ƒë√°nh gi√° & Kh√¥ng th·ªÉ ƒë√°nh gi√°  

                        

            M·ªánh ƒë·ªÅ th√¥ng tin: {statement}  

            B·∫±ng ch·ª©ng:  
            {evidence}  

            ### **H√£y tr·∫£ l·ªùi ch√∫ √Ω c√°c r√†ng bu·ªôc ph√≠a du·ªõi:**  
            - T·ªïng H·ª£p Cu·ªëi C√πng: [T√≥m t·∫Øt th√¥ng tin ƒë√£ ki·ªÉm tra ƒë·ªÉ ƒë∆∞a ra k·∫øt lu·∫≠n cu·ªëi c√πng v·ªÅ ch·ªß ƒë·ªÅ.]  
            - K·∫øt lu·∫≠n: [H·ªó tr·ª£/M√¢u thu·∫´n/Trung l·∫≠p]  
            - Ph√¢n t√≠ch b·∫±ng ch·ª©ng: [C√°c d·∫´n ch·ª©ng tr√™n c√≥ m·ªëi li√™n h·ªá nh∆∞ th·∫ø n√†o trong vi·ªác ƒë∆∞a ra k·∫øt lu·∫≠n v·ªÅ v·∫•n ƒë·ªÅ ng∆∞·ªùi d√πng t√¨m hi·ªÉu]
            - M·ª©c ƒë·ªô tin c·∫≠y: [V√≠ d·ª•: A1, B3, D5] v√† ch√∫ th√≠ch c·ªßa CH√çNH X√ÅC NH∆Ø DANH S√ÅCH √ù nghƒ©a c·ªßa c√°c t·ªï h·ª£p ƒë√°nh gi√°  [v√≠ d·ª•: A1 - ƒê√°ng Tin C·∫≠y & ƒê√£ ƒê∆∞·ª£c X√°c Minh]   
            - Gi·∫£i th√≠ch: [Gi·∫£i th√≠ch ng·∫Øn g·ªçn v·ªÅ l√Ω do c·ªßa b·∫°n, c√≥ ƒë·ªÅ c·∫≠p ƒë·∫øn ngu·ªìn b·∫±ng ch·ª©ng v√† m·ª©c ƒë·ªô tin c·∫≠y c·ªßa ch√∫ng.]  
            - L·ªùi khuy√™n cho ng∆∞·ªùi d√πng v·ªÅ c√°ch nh√¨n nh·∫≠n hi·ªán t·∫°i: [M·ªôt l·ªùi khuy√™n ng·∫Øn g·ªçn]  
            - Danh s√°ch c√°c d·∫´n ch·ª©ng (m·ªói b√†i b√°o l√† m·ªôt string):  
                [S·ªë th·ª© t·ª± b√†i b√°o ]: Ti√™u ƒë·ªÅ - ngu·ªìn -  [url] \n
                [S·ªë th·ª© t·ª± b√†i b√°o ]: Ti√™u ƒë·ªÅ - ngu·ªìn -  [url] \n
            ....

            L∆ØU √ù: Tr∆∞·ªùng: [M·ª©c ƒë·ªô tin c·∫≠y] CH·ªà D∆Ø·ª¢C TR·∫¢ L·∫†I M·ªòT GI√Å TR·ªä K·∫æT QU·∫¢ DUY NH·∫§T
            ### **V√≠ d·ª• c√°ch ch√®n li√™n k·∫øt:**  
            - "B·∫±ng ch·ª©ng t·ª´ [S·ªë th·ª© t·ª± b√†i b√°o] cho th·∫•y r·∫±ng..."  
            - "Theo th√¥ng tin t·ª´ b√†i vi·∫øt n√†y [S·ªë th·ª© t·ª± b√†i b√°o], ..."  

            **V√≠ d·ª• ph√π h·ª£p c·ªßa ƒë·ªãnh d·∫°ng "key": "value" c·ªßa json, n·∫øu c√≥ d·∫•u ngo·∫∑c k√©p (") trong n·ªôi dung c·ªßa value h√£y ƒë·ªïi th√†nh d·∫•u ngo·∫∑c ƒë∆°n (') ƒë·ªÉ ƒë√∫ng ƒë·ªãnh d·∫°ng json, c·∫•m d√πng d·∫•u ngo·∫∑c k√©p (") khi vi·∫øt ph·∫ßn value:**
            "T·ªïng H·ª£p Cu·ªëi C√πng": "C√°c b·∫±ng ch·ª©ng ƒë∆∞·ª£c cung c·∫•p kh√¥ng ch·ª©a b·∫•t k·ª≥ th√¥ng tin n√†o li√™n quan ƒë·∫øn \"C√¥ng ty G\" hay l·ª£i nhu·∫≠n c·ªßa c√¥ng ty n√†y trong nƒÉm 2025, c≈©ng nh∆∞ kh√¥ng cung c·∫•p d·ªØ li·ªáu so s√°nh l·ª£i nhu·∫≠n c·ªßa c√°c c√¥ng ty trong ng√†nh t√†i ch√≠nh ƒë·ªÉ x√°c ƒë·ªãnh c√¥ng ty c√≥ l·ª£i nhu·∫≠n cao nh·∫•t trong nƒÉm ƒë√≥.",
            "K·∫øt lu·∫≠n": "Trung l·∫≠p",
            "Ph√¢n t√≠ch b·∫±ng ch·ª©ng": "C√°c b·∫±ng ch·ª©ng ƒë∆∞·ª£c cung c·∫•p bao g·ªìm c√°c b√†i b√°o t·ª´ VnExpress v√† VnEconomy, l√† c√°c ngu·ªìn tin t·ª©c kinh doanh uy t√≠n. Tuy nhi√™n, n·ªôi dung c·ªßa ch√∫ng kh√¥ng li√™n quan tr·ª±c ti·∫øp ho·∫∑c gi√°n ti·∫øp ƒë·∫øn m·ªánh ƒë·ªÅ \"C√¥ng ty G c√≥ l·ª£i nhu·∫≠n cao nh·∫•t trong ng√†nh t√†i ch√≠nh nƒÉm 2025\". B·∫±ng ch·ª©ng [1] th·∫£o lu·∫≠n v·ªÅ k·∫ø ho·∫°ch v√† k·∫øt qu·∫£ kinh doanh c·ªßa HDBank trong nƒÉm 2023 v√† 2024. B·∫±ng ch·ª©ng [2] ƒë∆∞a ra ƒë√°nh gi√° v·ªÅ c√°c k√™nh ƒë·∫ßu t∆∞ ti·ªÅm nƒÉng trong nƒÉm 2024. B·∫±ng ch·ª©ng [3] gi·∫£i th√≠ch v·ªÅ ch·ªâ s·ªë PEG ƒë·ªÉ ƒë·ªãnh gi√° c·ªï phi·∫øu. B·∫±ng ch·ª©ng [4] ph√¢n t√≠ch k·ª≥ v·ªçng c·ªßa nh√† ƒë·∫ßu t∆∞ n∆∞·ªõc ngo√†i v·ªÅ ch√≠nh s√°ch kinh t·∫ø Vi·ªát Nam, ch·ªß y·∫øu t·∫≠p trung v√†o nƒÉm 2023 v√† b·ªëi c·∫£nh vƒ© m√¥. Kh√¥ng c√≥ b·∫±ng ch·ª©ng n√†o ƒë·ªÅ c·∫≠p ƒë·∫øn \"C√¥ng ty G\" ho·∫∑c cung c·∫•p d·ªØ li·ªáu l·ª£i nhu·∫≠n d·ª± ki·∫øn ho·∫∑c th·ª±c t·∫ø cho nƒÉm 2025 c·ªßa b·∫•t k·ª≥ c√¥ng ty t√†i ch√≠nh n√†o, ƒë·∫∑c bi·ªát l√† d·ªØ li·ªáu so s√°nh ƒë·ªÉ x√°c ƒë·ªãnh c√¥ng ty d·∫´n ƒë·∫ßu v·ªÅ l·ª£i nhu·∫≠n.",
            "M·ª©c ƒë·ªô tin c·∫≠y": "B5 - ƒê√°ng Tin C·∫≠y (Ngu·ªìn) v√† Kh√¥ng Th·ªÉ ƒê√°nh Gi√° (Th√¥ng tin li√™n quan ƒë·∫øn m·ªánh ƒë·ªÅ)",
            "Gi·∫£i th√≠ch": "ƒê√°nh gi√° m·ª©c ƒë·ªô tin c·∫≠y l√† B5. C√°c ngu·ªìn tin VnExpress v√† VnEconomy l√† c√°c b√°o ƒëi·ªán t·ª≠ c√≥ uy t√≠n v√† ƒë∆∞·ª£c c√¥ng nh·∫≠n trong lƒ©nh v·ª±c kinh doanh v√† kinh t·∫ø t·∫°i Vi·ªát Nam (ƒë√°ng tin c·∫≠y - B). Tuy nhi√™n, n·ªôi dung c·ªßa t·∫•t c·∫£ c√°c b·∫±ng ch·ª©ng [1], [2], [3], [4] ƒë·ªÅu kh√¥ng ch·ª©a b·∫•t k·ª≥ th√¥ng tin n√†o v·ªÅ \"C√¥ng ty G\" ho·∫∑c d·ªØ li·ªáu l·ª£i nhu·∫≠n c·ªßa c√°c c√¥ng ty trong ng√†nh t√†i ch√≠nh d·ª± ki·∫øn cho nƒÉm 2025. Do ƒë√≥, d·ª±a tr√™n c√°c b·∫±ng ch·ª©ng n√†y, t√≠nh ch√≠nh x√°c c·ªßa m·ªánh ƒë·ªÅ \"C√¥ng ty G c√≥ l·ª£i nhu·∫≠n cao nh·∫•t trong ng√†nh t√†i ch√≠nh nƒÉm 2025\" ho√†n to√†n \"kh√¥ng th·ªÉ ƒë√°nh gi√°\".",
            "L·ªùi khuy√™n cho ng∆∞·ªùi d√πng v·ªÅ c√°ch nh√¨n nh·∫≠n hi·ªán t·∫°i": "C√°c b·∫±ng ch·ª©ng hi·ªán c√≥ kh√¥ng cung c·∫•p th√¥ng tin n√†o ƒë·ªÉ x√°c minh ho·∫∑c b√°c b·ªè m·ªánh ƒë·ªÅ v·ªÅ l·ª£i nhu·∫≠n c·ªßa \"C√¥ng ty G\" trong nƒÉm 2025. ƒê·ªÉ c√≥ ƒë∆∞·ª£c th√¥ng tin ƒë√°ng tin c·∫≠y v·ªÅ l·ª£i nhu·∫≠n c·ªßa c√°c c√¥ng ty t√†i ch√≠nh v√† x·∫øp h·∫°ng c·ªßa h·ªç trong t∆∞∆°ng lai, b·∫°n c·∫ßn t√¨m ki·∫øm c√°c b√°o c√°o ph√¢n t√≠ch chuy√™n s√¢u t·ª´ c√°c c√¥ng ty ch·ª©ng kho√°n uy t√≠n, b√°o c√°o t√†i ch√≠nh d·ª± ki·∫øn c·ªßa c√°c c√¥ng ty, ho·∫∑c c√°c ngu·ªìn tin t·ª©c t√†i ch√≠nh chuy√™n ng√†nh ƒë∆∞a ra d·ª± b√°o c·ª• th·ªÉ cho nƒÉm 2025.",
            "Danh s√°ch c√°c d·∫´n ch·ª©ng": **ngo·∫∑c nh·ªçn ·ªü ƒë√¢y**
                "[1]":    
                        **ngo·∫∑c nh·ªçn ·ªü ƒë√¢y**
                            "title": "HDBank n√¢ng m·ª©c chia c·ªï t·ª©c l√™n 30%", 
                            "publisher": "B√°o VnExpress Kinh doanh", 
                            "url": "https://vnexpress.net/hdbank-nang-muc-chia-co-tuc-len-30-4737638.html" **ngo·∫∑c nh·ªçn ·ªü ƒë√¢y**
                        ,
                "[2]":    **ngo·∫∑c nh·ªçn ·ªü ƒë√¢y**
                        
                            "title": "L·ª±a ch·ªçn k√™nh ƒë·∫ßu t∆∞ n√†o trong nƒÉm 2024", 
                            "publisher": "B√°o VnExpress Kinh doanh", 
                            "url": "https://vnexpress.net/lua-chon-kenh-dau-tu-nao-trong-nam-2024-4699524.html"**ngo·∫∑c nh·ªçn ·ªü ƒë√¢y**
                        ,   
                "[3]":    **ngo·∫∑c nh·ªçn ·ªü ƒë√¢y**
                        
                            "title": "Ch·ªâ s·ªë PEG l√† g√¨?", 
                            "publisher": "B√°o VnExpress Kinh doanh", 
                            "url": "https://vnexpress.net/chi-so-peg-la-gi-4861277.html"**ngo·∫∑c nh·ªçn ·ªü ƒë√¢y**
                        ,
                "[4]":    **ngo·∫∑c nh·ªçn ·ªü ƒë√¢y**
                        
                            "title": "Nh√† ƒë·∫ßu t∆∞ n∆∞·ªõc ngo√†i k·ª≥ v·ªçng g√¨ v·ªÅ nh·ªØng ph·∫£n ·ª©ng ch√≠nh s√°ch c·ªßa Vi·ªát Nam?", 
                            "publisher": "Nh·ªãp s·ªëng kinh t·∫ø Vi·ªát Nam & Th·∫ø gi·ªõi", 
                            "url": "https://vneconomy.vn/nha-dau-tu-nuoc-ngoai-ky-vong-gi-ve-nhung-phan-ung-chinh-sach-cua-viet-nam.htm" **ngo·∫∑c nh·ªçn ·ªü ƒë√¢y**
            **ngo·∫∑c nh·ªçn ·ªü ƒë√¢y**            
            

    

            H√£y ƒë·∫£m b·∫£o tr·∫£ l·ªùi gi·ªëng nh∆∞ v√≠ d·ª•, nh∆∞ng kh√¥ng ƒë·ªÉ n·ªôi dung v√≠ d·ª• ·∫£nh h∆∞·ªüng ƒë·∫øn ƒë√°nh gi√°.
            
            """
        try:
            response = self.model.generate_content(prompt)
            if not hasattr(response, "text") or not response.text:
                print("‚ö†Ô∏è Warning: AI model returned empty response for evidence analysis.")
                return "Kh√¥ng c√≥ ph·∫£n h·ªìi t·ª´ AI."

            return response.text

        except Exception as e:
            print(f"‚ùå Error in analyze_evidence: {e}")
            return "ƒê√£ x·∫£y ra l·ªói khi ph√¢n t√≠ch b·∫±ng ch·ª©ng."
    
    def generate_bias_analysis(self, article : str):
            """
            Generate a qualitative bias and logical fallacy analysis on the article,
            specifying the types of bias and logical fallacies to focus on.
            
            Parameters:
                article (str): The article content to analyze.
    
            Returns:
                json: The formatted prompt with "key": "value".
            """
            # H√£y ph√¢n t√≠ch b√†i vi·∫øt sau theo ƒë·ªãnh d·∫°ng json ƒë·ªÉ x√°c ƒë·ªãnh c√°c thi√™n ki·∫øn v√† l·ªói l·∫≠p lu·∫≠n c√≥ th·ªÉ c√≥.  
            prompt = f"""
                B·∫°n l√† m·ªôt nh√† b√°o ph√¢n t√≠ch ph·∫£n bi·ªán, chuy√™n ƒë√°nh gi√° ƒë·ªô ch√≠nh x√°c v√† kh√°ch quan c·ªßa th√¥ng tin.  
                
                
                - **Kh√¥ng ch·ªâ d·ª±a v√†o t·ª´ kh√≥a**, h√£y ƒë√°nh gi√° ng·ªØ c·∫£nh v√† c√°ch l·∫≠p lu·∫≠n ƒë·ªÉ nh·∫≠n di·ªán thi√™n ki·∫øn ho·∫∑c l·ªói logic.  
                - N·∫øu b√†i vi·∫øt trung l·∫≠p, h√£y k·∫øt lu·∫≠n trung l·∫≠p. N·∫øu c√≥ thi√™n ki·∫øn ho·∫∑c l·ªói l·∫≠p lu·∫≠n, h√£y ƒë√°nh gi√° m·ª©c ƒë·ªô ·∫£nh h∆∞·ªüng.  
                
                **Vi·∫øt Ch√∫ √Ω c√°c r√†ng bu·ªôc ph√≠a du·ªõi ,ch·ªâ bao g·ªìm n·ªôi dung ph√¢n t√≠ch m√† KH√îNG TH√äM GI·∫¢I TH√çCH:**  

                - Lo·∫°i thi√™n ki·∫øn: [Ch√≠nh tr·ªã, gi·ªõi t√≠nh, vƒÉn h√≥a, thi√™n ki·∫øn x√°c nh·∫≠n, v.v.]  
                - M·ª©c ƒë·ªô ·∫£nh h∆∞·ªüng: [Nh·∫π, v·ª´a, nghi√™m tr·ªçng]  
                - Ph√¢n t√≠ch ng·∫Øn g·ªçn: [Gi·∫£i th√≠ch thi√™n ki·∫øn trong t·ªëi ƒëa 200 t·ª´, d·ª±a tr√™n ng·ªØ c·∫£nh v√† l·∫≠p lu·∫≠n c·ªßa b√†i vi·∫øt]  

                ---  
                **C√¢u h·ªèi ph·∫£n bi·ªán ƒë·ªÉ gi√∫p ng∆∞·ªùi ƒë·ªçc c√≥ g√≥c nh√¨n kh√°ch quan h∆°n:**  
                (H√£y ƒë∆∞a ra 3‚Äì5 c√¢u h·ªèi theo ph∆∞∆°ng ph√°p Socrates, khuy·∫øn kh√≠ch ng∆∞·ªùi ƒë·ªçc suy nghƒ© s√¢u h∆°n v·ªÅ l·∫≠p lu·∫≠n trong b√†i vi·∫øt)  
            
                **Tr·∫£ v·ªÅ ƒë·ªãnh d·∫°ng nh∆∞ b√™n d∆∞·ªõi - "key": "value" c·ªßa json format, n·∫øu c√≥ d·∫•u ngo·∫∑c k√©p (") trong n·ªôi dung c·ªßa value h√£y ƒë·ªïi th√†nh d·∫•u ngo·∫∑c ƒë∆°n (') ƒë·ªÉ ƒë√∫ng ƒë·ªãnh d·∫°ng json, c·∫•m d√πng d·∫•u ngo·∫∑c k√©p (") khi vi·∫øt ph·∫ßn n·ªôi dung trong value:**
                "Lo·∫°i thi√™n ki·∫øn": "L·ªói l·∫≠p lu·∫≠n: M√¢u thu·∫´n tr·ª±c ti·∫øp",
                "M·ª©c ƒë·ªô ·∫£nh h∆∞·ªüng": "Nghi√™m tr·ªçng",
                "Ph√¢n t√≠ch ng·∫Øn g·ªçn": "C√¢u n√≥i ch·ª©a m√¢u thu·∫´n logic tr·ª±c ti·∫øp: \"C√¥ng ty E th·∫•t b·∫°i th·∫£m h·∫°i\" v√† \"v·∫´n l√† kho·∫£n ƒë·∫ßu t∆∞ t·ªët nh·∫•t\". Theo ƒë·ªãnh nghƒ©a th√¥ng th∆∞·ªùng trong t√†i ch√≠nh, hai tr·∫°ng th√°i n√†y kh√≥ c√≥ th·ªÉ t·ªìn t·∫°i ƒë·ªìng th·ªùi m·ªôt c√°ch h·ª£p l√Ω. S·ª± m√¢u thu·∫´n n√†y l√†m suy y·∫øu nghi√™m tr·ªçng t√≠nh h·ª£p l√Ω v√† ƒë√°ng tin c·∫≠y c·ªßa nh·∫≠n ƒë·ªãnh, cho th·∫•y s·ª± thi·∫øu r√µ r√†ng trong ti√™u ch√≠ ƒë√°nh gi√° ho·∫∑c l·ªói trong l·∫≠p lu·∫≠n, khi·∫øn ng∆∞·ªùi ƒë·ªçc kh√≥ hi·ªÉu v√† ch·∫•p nh·∫≠n th√¥ng tin.",
                "C√¢u h·ªèi ph·∫£n bi·ªán": [
                    "Nh·ªØng ti√™u ch√≠ c·ª• th·ªÉ n√†o ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ x√°c ƒë·ªãnh ƒë√¢y l√† \"kho·∫£n ƒë·∫ßu t∆∞ t·ªët nh·∫•t\" trong khi c√¥ng ty ƒë∆∞·ª£c m√¥ t·∫£ l√† \"th·∫•t b·∫°i th·∫£m h·∫°i\"?",
                    "L√†m th·∫ø n√†o ƒë·ªÉ dung h√≤a nh·∫≠n ƒë·ªãnh v·ªÅ s·ª± "th·∫•t b·∫°i th·∫£m h·∫°i" v·ªõi kh·∫≥ng ƒë·ªãnh v·ªÅ hi·ªáu qu·∫£ ƒë·∫ßu t∆∞ \"t·ªët nh·∫•t\"?",
                    "Li·ªáu ƒë·ªãnh nghƒ©a v·ªÅ \"th·∫•t b·∫°i\" ho·∫∑c \"kho·∫£n ƒë·∫ßu t∆∞ t·ªët nh·∫•t\" ƒëang ƒë∆∞·ª£c s·ª≠ d·ª•ng c√≥ kh√°c bi·ªát so v·ªõi th√¥ng l·ªá t√†i ch√≠nh kh√¥ng?",
                    "C√≥ th√¥ng tin ho·∫∑c b·ªëi c·∫£nh n√†o b·ªã thi·∫øu c√≥ th·ªÉ gi√∫p gi·∫£i th√≠ch s·ª± m√¢u thu·∫´n r√µ r√†ng trong nh·∫≠n ƒë·ªãnh n√†y kh√¥ng?"
                ]
                
                B√†i vi·∫øt c·∫ßn ph√¢n t√≠ch:  
                \"\"\"  
                {article}  
                \"\"\"
            """
            try:
                response = self.model.generate_content(prompt)
                if not hasattr(response, "text") or not response.text:
                    print("‚ö†Ô∏è Warning: Empty response from AI model.")
                    return []

                analysis = response.text
                return analysis
            
            except Exception as e:
                print(f"‚ùå Error in generate_search_queries: {e}")
                return []

    def understanding_the_question(self, query):
        """
        Method 1:  Reasoning - Understand the User Query using Gemini.

        This method sends the user's query to Gemini and asks it to
        explain its reasoning process for understanding the question.
        The reasoning is captured and returned, but not printed to the user directly.
        
        Args:
            query (str): The user's query.

        Returns:
            str: A string representing Gemini's reasoning process for understanding the query.
            Returns None if there's an error communicating with Gemini.
        """
        try:
            
            prompt = f"""
            
            B·∫°n l√† m·ªôt chuy√™n gia ph√¢n t√≠ch d·ªØ li·ªáu c√≥ nhi·ªám v·ª• tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n d·ªØ li·ªáu t√¨m ki·∫øm.  
            Tr∆∞·ªõc khi t·∫°o c√¢u tr·∫£ l·ªùi, **H√£y hi·ªÉu v√† suy lu·∫≠n v·ªÅ √Ω nghƒ©a v√† tr·ªçng t√¢m c·ªßa c√¢u h·ªèi** r·ªìi tr·∫£ l·∫°i
            k·∫øt qu·∫£ ƒë·∫ßu ra\n
            \n
            *K·∫øt qu·∫£ qu√° tr√¨nh suy lu·∫≠n*\n 
            - X√°c ƒë·ªãnh v·∫•n ƒë·ªÅ ch√≠nh c·∫ßn ph·∫£i tr·∫£ l·ªùi: [v·∫•n ƒë·ªÅ 1, v·∫•n ƒë·ªÅ 2, etc.]\n 
            - X√°c ƒë·ªãnh t·ª´ kh√≥a t√¨m ki·∫øm t·ªëi ∆∞u. [T·ª´ kho√° 1, t·ª´ kho√° 2, etc.]\n 
            - X√°c ƒë·ªãnh gi·∫£ ƒë·ªãnh ti·ªÅm ·∫©n (n·∫øu c√≥): [Gi·∫£ thuy·∫øt 1, Gi·∫£ thuy·∫øt 2, etc.] \n 
            \n 
            C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: '{query}' \n 
            
            """
            response = self.model.generate_content(prompt)
            reasoning = response.text
            return reasoning
        
        except Exception as e:
            print(f"Error in reasoning method: {e}")
        return None
        
    def synthesize_and_summarize(self, query, reasoning, evidence):
        """
        Method 2: Synthesis and Summarization - Generate a clear answer based on reasoning.

        This method takes the original user query and the reasoning obtained from
        reason_about_query(). It uses Gemini to synthesize evidence (implicitly from its knowledge)
        and summarize it into a clear and concise answer, guided by the provided reasoning.

        Args:
            query (str): The original user query.\n
            reasoning (str): The reasoning process obtained from reason_about_query().\n
            evidence (list[str]]): The list of evidence main_text\n

        Returns:
            str: A clear and concise summarized answer to the user's query.\n
            Returns None if there's an error communicating with Gemini.\n
        """
        if reasoning is None:
            return "Could not determine reasoning for the query. Please try again."

        try:     
            prompt = f"""
            B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n ph√¢n t√≠ch, t·ªïng h·ª£p v√† t√≥m t·∫Øt th√¥ng tin ƒë·ªÉ tr·∫£ l·ªùi truy v·∫•n c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch r√µ r√†ng v√† ch√≠nh x√°c.\n

            ## **Nhi·ªám v·ª• c·ªßa b·∫°n**:
            D·ª±a tr√™n truy v·∫•n c·ªßa ng∆∞·ªùi d√πng, l·∫≠p lu·∫≠n ƒë√£ c√≥, v√† danh s√°ch b·∫±ng ch·ª©ng, h√£y t·ªïng h·ª£p c√¢u tr·∫£ l·ªùi m·ªôt c√°ch logic, d·ªÖ hi·ªÉu, v√† ng·∫Øn g·ªçn.
            \n
            ---\n

            ## **D·ªØ li·ªáu ƒë·∫ßu v√†o**:\n
            **Truy v·∫•n**: {query}\n 
            **L·∫≠p lu·∫≠n h·ªó tr·ª£**: {reasoning}\n
            **B·∫±ng ch·ª©ng**:  {evidence}\n 

            ---

            ## **Y√™u c·∫ßu ƒë·ªëi v·ªõi c√¢u tr·∫£ l·ªùi**:\n
            1. **T√≥m t·∫Øt ng·∫Øn g·ªçn nh∆∞ng ƒë·∫ßy ƒë·ªß**:\n  
            - Kh√¥ng ch·ªâ tr√≠ch d·∫´n m√† ph·∫£i t·ªïng h·ª£p th√¥ng tin t·ª´ b·∫±ng ch·ª©ng.\n   
            - ƒê·∫£m b·∫£o c√¢u tr·∫£ l·ªùi c√≥ √Ω nghƒ©a ngay c·∫£ khi kh√¥ng c√≥ ƒë·∫ßy ƒë·ªß ng·ªØ c·∫£nh ban ƒë·∫ßu. \n  
            \n
            2.**S·ª≠ d·ª•ng l·∫≠p lu·∫≠n h·ª£p l√Ω**: \n  
            - T·∫≠n d·ª•ng reasoning ƒë·ªÉ ƒë∆∞a ra k·∫øt lu·∫≠n logic. \n  
            - N·∫øu b·∫±ng ch·ª©ng m√¢u thu·∫´n, h√£y ch·ªâ ra ƒëi·ªÉm kh√°c bi·ªát thay v√¨ ƒë∆∞a ra m·ªôt c√¢u tr·∫£ l·ªùi phi·∫øn di·ªán.  \n 
            \n 
            3.**ƒê·ªãnh d·∫°ng c√¢u tr·∫£ l·ªùi**:\n 
            **T√≥m t·∫Øt cu·ªëi c√πng**: [T√≥m t·∫Øt c√¢u tr·∫£ l·ªùi d·ª±a tr√™n b·∫±ng ch·ª©ng]\n   
            **Ngu·ªìn tham kh·∫£o**: [Danh s√°ch ngu·ªìn th√¥ng tin & n·∫øu c√≥ th·ªÉ h√£y ƒë√≠nh k√®m link ngu·ªìn]  \n 
            \n 
            üéØ **L∆∞u √Ω**:\n 
            - N·∫øu kh√¥ng c√≥ ƒë·ªß b·∫±ng ch·ª©ng, h√£y n√™u r√µ ƒëi·ªÅu ƒë√≥ thay v√¨ ƒë∆∞a ra c√¢u tr·∫£ l·ªùi suy ƒëo√°n.\n 
            - N·∫øu c√≥ l·ªói ho·∫∑c kh√¥ng th·ªÉ t·ªïng h·ª£p ƒë∆∞·ª£c c√¢u tr·∫£ l·ªùi, tr·∫£ v·ªÅ `"Kh√¥ng th·ªÉ ƒë∆∞a ra k·∫øt lu·∫≠n r√µ r√†ng."`\n 
            - V√≠ d·ª• c√°ch ch√®n li√™n k·∫øt:
                - "B·∫±ng ch·ª©ng t·ª´ [ngu·ªìn n√†y](URL) cho th·∫•y r·∫±ng..."  
                - "Theo th√¥ng tin t·ª´ b√†i vi·∫øt n√†y ([link](URL)), ..."  
            """
            response = self.model.generate_content(prompt)
            summary = response.text
            return summary
        except Exception as e:
            print(f"Error in synthesis and summarization method: {e}")
            return None
    
    def filter_rank(self,query, valid_articles):
        corpus = [str(query)] + valid_articles # Set the query to a list of string to prevent out-of-bound index dues to the corpus-size >= valid_articles-size
        # Step 1: TF-IDF Vectorization
        vectorizer = TfidfVectorizer(stop_words='english')
        tfidf_matrix = vectorizer.fit_transform(corpus)

        # Step 2: Compute cosine similarity
        query_vector = tfidf_matrix[0]  # first is query
        candidate_vectors = tfidf_matrix[1:]

        similarities = cosine_similarity(query_vector, candidate_vectors).flatten()

        # Step 3: Rank answers
        ranked_indices = similarities.argsort()[::-1]  # Descending order
        ranked_articles = [valid_articles[i] for i in ranked_indices]
        return ranked_articles

    def generate_tags(self, article, predefined_tags=None):
        tags = []
        
        # Create the prompt
        if predefined_tags:
            predefined_str = ', '.join(predefined_tags)
            prompt = f"""
            cho m·ªôt b√†i vi·∫øt sau: {article}

            h√£y t·∫°o m·ªôt danh s√°ch th·∫ª [CH·ªà ƒê∆Ø·ª¢C C√ì ƒê√öNG 4 TH·∫∫] (tag) v·ªõi y√™u c·∫ßu sau: 
            C√°c th·∫ª t·∫°o ra ph·∫£i bao g·ªìm n·ªôi dung ch√≠nh c·ªßa b√†i t√°o v√† kh√¥ng ƒë∆∞·ª£c tr√πng l·∫∑p v·ªõi nhau. 
            Ch·ªâ ch·ªçn ƒë√∫ng 1 th·∫ª t·ª´ danh s√°ch sau: {predefined_str}, v√† t·∫°o th√™m 1 ƒë·∫øn 3 th·∫ª li√™n quan kh√°c. 
            Xu·∫•t k·∫øt qu·∫£ theo ƒë·ªãnh d·∫°ng sau, ch·ªâ bao g·ªìm n·ªôi dung ph√¢n t√≠ch m√† kh√¥ng th√™m gi·∫£i th√≠ch ho·∫∑c bi·ªÉu c·∫£m d∆∞ th·ª´a:  
            Nghi√™m c·∫•m th√™m c√¢u ƒë·ªám nh∆∞ "Danh s√°ch th·∫ª bao g·ªìm:", "C√°c th·∫ª bao g·ªìm:", v.v.  
            Nghi√™m c·∫•m th√™m m·ªôt s·ªë ch√∫ th√≠ch kh√¥ng c·∫ßn thi·∫øt nh∆∞: (assuming current year is 2024 as article say "this year")
            ƒê·ªò D√ÄI C·ª¶A M·ªòT TH·∫∫ CH·ªà ƒê∆Ø·ª¢C T·ªêI ƒêA 3 T·ª™. (V√≠ d·ª• h·ª£p l·ªá: "Ch√≠nh Tr·ªã", "Kinh T·∫ø", "2022") - V√ç D·ª§ KH√îNG H·ª¢P L·ªÜ: "Ch√≠nh Tr·ªã v√† Kinh T·∫ø", "Ch√≠nh Tr·ªã v√† Kinh T·∫ø v√† 2022")
            ** V√≠ d·ª• ƒë·ªãnh d·∫°ng: 
                th·∫ª1, th·∫ª2, th·∫ª3
            ---
            M·∫´u V√≠ D·ª•: (Kh√¥ng ph·∫£i k·∫øt qu·∫£ th·ª±c t·∫ø)
                Ch√≠nh Tr·ªã, Kinh T·∫ø, 2022
            """
        else:
            prompt = f"""
            cho m·ªôt b√†i vi·∫øt sau: {article}

            h√£y t·∫°o m·ªôt danh s√°ch th·∫ª [CH·ªà ƒê∆Ø·ª¢C C√ì ƒê√öNG 4 TH·∫∫] (tag) d·ª±a tr√™n n·ªôi dung c·ªßa b√†i vi·∫øt ƒë∆∞·ª£c cung c·∫•p v·ªõi y√™u c·∫ßu sau:
            C√°c th·∫ª t·∫°o ra ph·∫£i bao g·ªìm n·ªôi dung ch√≠nh c·ªßa b√†i t√°o v√† kh√¥ng ƒë∆∞·ª£c tr√πng l·∫∑p v·ªõi nhau. 
            Xu·∫•t k·∫øt qu·∫£ theo ƒë·ªãnh d·∫°ng sau, ch·ªâ bao g·ªìm n·ªôi dung danh s√°ch th·∫ª m√† kh√¥ng th√™m gi·∫£i th√≠ch, h·ªìi ƒë√°p ho·∫∑c bi·ªÉu c·∫£m d∆∞ th·ª´a:
            Nghi√™m c·∫•m th√™m c√¢u ƒë·ªám nh∆∞ "Danh s√°ch th·∫ª bao g·ªìm:", "C√°c th·∫ª bao g·ªìm:", v.v.  
            Nghi√™m c·∫•m th√™m m·ªôt s·ªë ch√∫ th√≠ch kh√¥ng c·∫ßn thi·∫øt nh∆∞: (assuming current year is 2024 as article say "this year")
            ƒê·ªò D√ÄI C·ª¶A M·ªòT TH·∫∫ CH·ªà ƒê∆Ø·ª¢C T·ªêI ƒêA 3 T·ª™. (V√≠ d·ª• h·ª£p l·ªá: "Ch√≠nh Tr·ªã", "Kinh T·∫ø", "2022") - V√ç D·ª§ KH√îNG H·ª¢P L·ªÜ: "Ch√≠nh Tr·ªã v√† Kinh T·∫ø", "Ch√≠nh Tr·ªã v√† Kinh T·∫ø v√† 2022")

            ** V√≠ d·ª• ƒë·ªãnh d·∫°ng: 
                th·∫ª1, th·∫ª2, th·∫ª3
            ---
            M·∫´u V√≠ D·ª•: (Kh√¥ng ph·∫£i k·∫øt qu·∫£ th·ª±c t·∫ø)
                Ch√≠nh Tr·ªã, Kinh T·∫ø, 2022
            """

        # Generate content from model
        response = self.model.generate_content(prompt)
        #print("res: ",response.text)
        raw_tags = response.text.strip().split(',')

        # Clean up tags
        for tag in raw_tags:
            cleaned_tag = tag.strip()
            if cleaned_tag:  # Avoid empty tags
                tags.append(cleaned_tag)

        # Optional: Ensure at least 1 predefined tag if required
        if predefined_tags:
            predefined_lower = [t.lower() for t in predefined_tags]
            has_predefined = any(tag.lower() in predefined_lower for tag in tags)

            if not has_predefined:
                fallback_tag = predefined_tags[0]  # pick the first predefined tag
                tags.insert(0, fallback_tag)

        return tags
    
    def generate_tags_batch(self, articles: List[str], predefined_tags: Optional[List[str]] = None) -> List[List[str]]:
        all_tags = []

        # Create the prompt for batch processing
        article_text = ''.join([f'B√†i vi·∫øt {i+1}: {article}\n' for i, article in enumerate(articles)])

        if predefined_tags:
            predefined_str = ', '.join(predefined_tags)
            prompt = (
                "cho c√°c b√†i vi·∫øt sau:\n\n"
                f"{article_text}\n"
                "H√£y t·∫°o m·ªôt danh s√°ch th·∫ª [CH·ªà ƒê∆Ø·ª¢C C√ì ƒê√öNG 4 TH·∫∫] cho m·ªói b√†i vi·∫øt v·ªõi y√™u c·∫ßu sau:\n"
                "- C√°c th·∫ª ph·∫£i bao g·ªìm n·ªôi dung ch√≠nh c·ªßa b√†i vi·∫øt v√† kh√¥ng ƒë∆∞·ª£c tr√πng l·∫∑p v·ªõi nhau.\n"
                f"- Ch·ªâ ch·ªçn ƒë√∫ng 1 th·∫ª t·ª´ danh s√°ch sau: {predefined_str}, v√† t·∫°o th√™m 1 ƒë·∫øn 3 th·∫ª li√™n quan kh√°c.\n"
                "- ƒê·ªô d√†i c·ªßa m·ªôt th·∫ª ch·ªâ ƒë∆∞·ª£c t·ªëi ƒëa 3 t·ª´.\n\n"
                "Xu·∫•t k·∫øt qu·∫£ theo ƒë·ªãnh d·∫°ng sau, ch·ªâ bao g·ªìm n·ªôi dung ph√¢n t√≠ch m√† kh√¥ng th√™m gi·∫£i th√≠ch ho·∫∑c bi·ªÉu c·∫£m d∆∞ th·ª´a:\n"
                "B√†i vi·∫øt 1: th·∫ª1, th·∫ª2, th·∫ª3, th·∫ª4\n"
                "B√†i vi·∫øt 2: th·∫ª1, th·∫ª2, th·∫ª3, th·∫ª4\n"
                "..."
            )
        else:
            prompt = (
                "cho c√°c b√†i vi·∫øt sau:\n\n"
                f"{article_text}\n"
                "H√£y t·∫°o m·ªôt danh s√°ch th·∫ª [CH·ªà ƒê∆Ø·ª¢C C√ì ƒê√öNG 4 TH·∫∫] cho m·ªói b√†i vi·∫øt v·ªõi y√™u c·∫ßu sau:\n"
                "- C√°c th·∫ª ph·∫£i bao g·ªìm n·ªôi dung ch√≠nh c·ªßa b√†i vi·∫øt v√† kh√¥ng ƒë∆∞·ª£c tr√πng l·∫∑p v·ªõi nhau.\n"
                "- ƒê·ªô d√†i c·ªßa m·ªôt th·∫ª ch·ªâ ƒë∆∞·ª£c t·ªëi ƒëa 3 t·ª´.\n\n"
                "Xu·∫•t k·∫øt qu·∫£ theo ƒë·ªãnh d·∫°ng sau, ch·ªâ bao g·ªìm n·ªôi dung ph√¢n t√≠ch m√† kh√¥ng th√™m gi·∫£i th√≠ch ho·∫∑c bi·ªÉu c·∫£m d∆∞ th·ª´a:\n"
                "B√†i vi·∫øt 1: th·∫ª1, th·∫ª2, th·∫ª3, th·∫ª4\n"
                "B√†i vi·∫øt 2: th·∫ª1, th·∫ª2, th·∫ª3, th·∫ª4\n"
                "..."
            )
        # Generate content from model
        response = self.model.generate_content(prompt)
        
        raw_responses = response.text.strip().split('\n')

        # Process each article's tags
        for raw_tags in raw_responses:
            tags = []
            if ':' in raw_tags:
                _, tag_str = raw_tags.split(':', 1)
                raw_tag_list = tag_str.strip().split(',')
                for tag in raw_tag_list:
                    cleaned_tag = tag.strip()
                    if cleaned_tag:  # Avoid empty tags
                        tags.append(cleaned_tag)

                # Optional: Ensure at least 1 predefined tag if required
                if predefined_tags:
                    predefined_lower = [t.lower() for t in predefined_tags]
                    has_predefined = any(tag.lower() in predefined_lower for tag in tags)

                    if not has_predefined and predefined_tags:
                        fallback_tag = predefined_tags[0]  # pick the first predefined tag
                        tags.insert(0, fallback_tag)

            all_tags.append(tags)

        return all_tags

    def generate_brief_descriptions_batch(self, articles: List[str]) -> List[str]:
        all_descriptions = []

        # Create the prompt for batch processing
        article_text = ''.join([f'B√†i vi·∫øt {i+1}: {article}\n' for i, article in enumerate(articles)])

        prompt = (
            "D∆∞·ªõi ƒë√¢y l√† m·ªôt lo·∫°t b√†i vi·∫øt:\n\n"
            f"{article_text}\n"
            "H√£y vi·∫øt m·ªôt ƒëo·∫°n m√¥ t·∫£ ng·∫Øn [CH·ªà ƒê∆Ø·ª¢C t·ª´ 10 ƒë·∫øn 50 t·ª´] cho m·ªói b√†i vi·∫øt. M√¥ t·∫£ c·∫ßn n√™u b·∫≠t n·ªôi dung ch√≠nh m·ªôt c√°ch s√∫c t√≠ch, r√µ r√†ng v√† kh√¥ng bao g·ªìm c√°c y·∫øu t·ªë d∆∞ th·ª´a ho·∫∑c l·∫∑p l·∫°i.\n"
            "ƒê·ªãnh d·∫°ng k·∫øt qu·∫£ nh∆∞ sau, ch·ªâ bao g·ªìm n·ªôi dung m√¥ t·∫£, kh√¥ng th√™m gi·∫£i th√≠ch ho·∫∑c bi·ªÉu c·∫£m:\n"
            "B√†i vi·∫øt 1: [m√¥ t·∫£ ng·∫Øn]\n"
            "B√†i vi·∫øt 2: [m√¥ t·∫£ ng·∫Øn]\n"
            "..."
        )

        # Generate content from model
        response = self.model.generate_content(prompt)
        raw_responses = response.text.strip().split('\n')

        # Process each article's short description
        for raw_desc in raw_responses:
            if ':' in raw_desc:
                _, desc_str = raw_desc.split(':', 1)
                cleaned_desc = desc_str.strip()
                all_descriptions.append(cleaned_desc)

        return all_descriptions

    def process_articles_in_batches(self, articles: List[str], predefined_tags: Optional[List[str]] = None, batch_size: int = 5):
        all_results = []
        total_articles = len(articles)
        for i in range(0, total_articles, batch_size):
            batch = articles[i:i + batch_size]
            batch_results = self.generate_tags_batch(batch, predefined_tags)
            all_results.extend(batch_results)

            # Implement rate limiting
            if (i + batch_size) < total_articles:
                time.sleep(6)  # Sleep for 6 seconds to maintain ~10 RPM

        return all_results

    def choose_the_batch_size(self,article_list):
        """
            Input: List of Article
            Output: the Ideal batch-size
            Dynamically decide the batch size based on the number of articles.
            If the number of articles is less than or equal to 5, process them all at once.
            Otherwise, process them in batches of 5.
            This function assumes that the `article_util` module has a method `process_articles_in_batches`
            that can handle the processing of articles in batches.
        """
        total_articles = len(article_list)
        
        if total_articles == 0:
            print("No articles to process!")
            return 0
        # Dynamic batch size decision:
        if total_articles <= 5:
            batch_size = total_articles  # Put all articles in one batch
        else:
            batch_size = 5  # Max allowed batch size
        
        return batch_size
    
    
