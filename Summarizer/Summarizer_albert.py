from dotenv import load_dotenv
import google.generativeai as genai
import numpy as np
# import torch
# import torch.nn.functional as F
# from torch.nn.utils.rnn import pad_sequence
import re
import nltk
from typing import List
import networkx as nx  # support the the Text-Rank Algorithm
import os
# import json
import concurrent.futures
import time
from ViFinanceCrawLib.QualAna.ArticleFactCheckUtility import ArticleFactCheckUtility
import queue
from tqdm import tqdm
# import subprocess

class SummarizerAlbert:
    def __init__(self, stopword_file="vietnamese-stopwords-dash.txt",
                extractive_model="cservan/multilingual-albert-base-cased-32k",  
                task='feature-extraction',  
                abstractive_model ='gemini-2.0-flash-lite'):

        # setting up with abstractive model
        load_dotenv()
        genai.configure(api_key=os.getenv("API_KEY"))
        self.abstractive_model = genai.GenerativeModel(abstractive_model)
        self.qa_utility = ArticleFactCheckUtility()

        # Get the directory of the current file (summarizer.py)
        current_dir = os.path.dirname(os.path.abspath(__file__))
        print("üìÇ Current dir:", current_dir)
        stopword_path = os.path.join(current_dir, stopword_file)
        resource = "punkt_tab"
        try:
            nltk.data.find(f'tokenizers/{resource}')
            print(f"{resource} is already installed.")
        except LookupError:
            print(f"{resource} not found. Downloading...")
            nltk.download(resource)
            print(f"{resource} downloaded successfully.")
        
        self.stopword = set([
            "v√†", "c·ªßa", "l√†", "·ªü", "trong", "c√≥", "ƒë∆∞·ª£c", "cho", "v·ªõi", "t·∫°i",
            "nh∆∞", "n√†y", "ƒë√≥", "m·ªôt", "c√°c", "nh·ªØng", "ƒë·ªÉ", "v√†o", "ra", "l√™n",
            "ngo√†i_ra", "th∆∞·ªùng_xuy√™n", "ng√†y_c√†ng", "b·ªÅn_v·ªØng", "chi·∫øn_l∆∞·ª£c", "qu·∫£n_l√Ω",
            "hi·ªáu_qu·∫£", "h∆°n_n·ªØa", "ƒë·∫∑c_bi·ªát", "b∆∞·ªõc", "nh·ªè", "kh√¥ng_ng·ª´ng", "theo",
            "hay", "ho·∫∑c", "t·ª´", "v·ªÅ", "l√™n", "xu·ªëng", "trong", "ngo√†i", "ra", "v√†o",
            "ƒë∆∞·ª£c", "ƒë√£", "l√†", "v·ªõi", "c·ªßa", "tr√™n", "d∆∞·ªõi", "gi·ªØa", "sau", "tr∆∞·ªõc",
            "khi", "n·∫øu", "th√¨", "m√†", "nh∆∞ng", "v√¨", "do", "n√™n", "c≈©ng", "ƒë·ªÉ", "cho"
        ])
        if not os.path.exists(stopword_path):
            print("Not exist the additional_file - use the default set of stopwords")
        else:
            try:
                with open(stopword_path, 'r', encoding='utf-8') as file:
                    file_stopwords = {line.strip() for line in file if line.strip()}
                self.stopword = self.stopword.union(file_stopwords)
            except Exception as e:
                print(f"Error reading stopwords file: {e}. Using default stopwords only.")
        
        self._tokenizer = None
        self._model = None
        self.model_root="/app/models/hub"
        self.extractive_model = extractive_model
        
        print("DONE")
        return

    @property
    def tokenizer(self):
        if self._tokenizer is None or self._model is None:
            self._tokenizer, self._model = self._load_extractive_model()
        return self._tokenizer

    @property
    def model(self):
        if self._model is None or self._tokenizer is None:
            self._tokenizer, self._model = self._load_extractive_model()
        return self._model

    def _load_extractive_model(self):
        print("üì¶ Importing transformers...")
        import time
        start = time.time()
        from transformers import AutoModel, AutoTokenizer  # do not move this out
        end = time.time()
        print(f"üïí transformers import time: {end - start:.2f} seconds")

        using_volume = os.path.isdir(self.model_root) and len(os.listdir(self.model_root)) > 0
        print(f"[INFO] using_volume = {using_volume}")

        start = time.time()
        if not using_volume:
            print("üîç Downloading model from Hugging Face...")
            tokenizer = AutoTokenizer.from_pretrained(self.extractive_model)
            model = AutoModel.from_pretrained(self.extractive_model)
        else:
            print("üîç Loading model from local volume...")
            albert_folder = self.find_model_folder("models--cservan--multilingual-albert-base-cased-32k", self.model_root)
            if albert_folder is None:
                raise Exception("‚ùå Model folder not found in volume.")
            tokenizer = AutoTokenizer.from_pretrained(albert_folder)
            model = AutoModel.from_pretrained(albert_folder)

        end = time.time()
        print(f"üïí Model + Tokenizer loaded in {end - start:.2f} seconds")

        model.eval()
        return tokenizer, model

    def find_model_folder(self, model_name, root_model_dir):
        for root, dirs, files in os.walk(root_model_dir):
            # Check if 'config.json' exists (which indicates a valid Hugging Face model folder)
            if "config.json" in files and model_name in root:
                return root
        return None
    
    def text_normalize(self, text):
        return text.strip().lower()  # Basic normalization (can be extended)

    def pre_processing(self, text):
      from nltk.tokenize import sent_tokenize, word_tokenize
      text = self.text_normalize(text)
      text = text.lower()
      # keep the some special token that bring semantic data
      text = re.sub(r'[^\w\s.!?%]', '', text)
      try:
          sentences = sent_tokenize(text)
          sentences = [s.strip() for s in sentences if s.strip()]
      except Exception as e:
          print(f"Error splitting sentences: {e}")
          return []
      processed_sentences = []
      for sentence in sentences:
          try:
              # tokenize
              tokens = word_tokenize(sentence)
              # Combine "number + %""
              new_tokens = []
              i = 0
              while i < len(tokens):
                  # Check if the current_token is number - next is %
                  if i + 1 < len(tokens) and tokens[i].isdigit() and tokens[i + 1] == "%":
                      new_tokens.append(tokens[i] + "%")  
                      i += 2  # Skip the token after the %
                  else:
                      new_tokens.append(tokens[i])
                      i += 1
              # Filter stopwords & normalization

              processed_tokens = [
                    token if token.endswith("%") else self.text_normalize(token)
                    for token in new_tokens
                    if token.endswith("%") or token.lower() not in self.stopword
                ]

              processed_sentences.append(" ".join(processed_tokens))

          except Exception as e:
              print(f"Error processing sentence: {e}")
              continue
      return processed_sentences
    
    def get_embeddings(self, texts: List[str]):
        import torch
        """
        Get sentence embeddings for a list of texts using a locally loaded transformer model.
        Uses dynamic batching and mean pooling over token embeddings.
        """
        device = next(self.model.parameters()).device  # Get model device
        num_texts = len(texts)
 
        # Dynamically choose a reasonable even batch size
        if num_texts <= 2:
            batch_size = num_texts
        else:
            batch_size = max(2, (num_texts // 4) * 2)  # Ensure it's even
 
        all_embeddings = []
 
        for i in range(0, num_texts, batch_size):
            batch = texts[i:i + batch_size]
 
            # Tokenize the batch with padding and truncation
            inputs = self.tokenizer(batch, return_tensors="pt", padding=True, truncation=True)
            inputs = {k: v.to(device) for k, v in inputs.items()}  # ‚¨ÖÔ∏è move to same device
 
            # Forward pass (no gradients needed)
            with torch.no_grad():
                outputs = self.model(**inputs)
 
            # Mean pooling over the token dimension (dim=1)
            batch_embeddings = outputs.last_hidden_state.mean(dim=1)  # shape: (batch_size, hidden_dim)
 
            all_embeddings.append(batch_embeddings)
 
        # Concatenate all batched embeddings into one tensor
        return torch.cat(all_embeddings, dim=0)
    
    def calculate_similarity_matrix(self, embeddings):
        import torch
        import torch.nn.functional as F
       
        # embeddings: [num_sentences, hidden_dim]
        embeddings_tensor = embeddings.clone().detach().float()
        normed = F.normalize(embeddings_tensor, p=2, dim=1)  # Normalize each vector using L2 norm
        similarity_matrix = torch.mm(normed, normed.T).numpy()  # Cosine sim = dot product of normalized vectors
        np.fill_diagonal(similarity_matrix, 0)
        return similarity_matrix

    def extractive_summary(self, sentences, num_sentences=3):
        device = next(self.model.parameters()).device
        if len(sentences) < num_sentences:
            print(f"Warning: Number of sentences ({len(sentences)}) is less than requested ({num_sentences}). Returning all sentences.")
            return sentences
        
        sentence_embeddings = self.get_embeddings(sentences)
        sentence_embeddings = [embedding.to(device) for embedding in sentence_embeddings]
        from torch.nn.utils.rnn import pad_sequence
        sentence_embeddings =  pad_sequence(sentence_embeddings, batch_first=True, padding_value=0) # Convert list to tensor
        similarity_matrix = self.calculate_similarity_matrix(sentence_embeddings)
 
        # Build graph and compute PageRank scores
        graph = nx.from_numpy_array(similarity_matrix)
        scores = nx.pagerank(graph, max_iter=100)
        
        # Select top-ranked sentences
        selected_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:num_sentences]
        return [sentences[i] for i in sorted(selected_indices)]
    #generate_extractive
    def generative_extractive(self, article_text):
        """
        Generate extractive summary with formatted output using regex pattern.
        Returns sentences in format: [c√¢u 1]. [c√¢u 2]. [c√¢u 3]. etc.
        """
        prompt = f"""
        B·∫°n l√† m·ªôt chuy√™n gia t√≥m t·∫Øt vƒÉn b·∫£n ti·∫øng Vi·ªát. H√£y th·ª±c hi·ªán c√°c y√™u c·∫ßu sau:

        1. Tr√≠ch xu·∫•t 5 c√¢u QUAN TR·ªåNG NH·∫§T t·ª´ vƒÉn b·∫£n ƒë∆∞·ª£c cung c·∫•p
        2. M·ªói c√¢u ph·∫£i l√† tr√≠ch d·∫´n nguy√™n vƒÉn, KH√îNG ƒê∆Ø·ª¢C VI·∫æT L·∫†I hay DI·ªÑN GI·∫¢I
        3. Tr·∫£ v·ªÅ k·∫øt qu·∫£ theo ƒë·ªãnh d·∫°ng CH√çNH X√ÅC nh∆∞ sau:
        [c√¢u 1]. [c√¢u 2]. [c√¢u 3]. [c√¢u 4]. [c√¢u 5].
        
        Quy t·∫Øc b·∫Øt bu·ªôc:
        - Ch·ªâ tr√≠ch xu·∫•t c√¢u nguy√™n vƒÉn
        - M·ªói c√¢u ƒë·∫∑t trong d·∫•u ngo·∫∑c vu√¥ng []
        - C√°c c√¢u ph√¢n c√°ch b·∫±ng d·∫•u ch·∫•m v√† kho·∫£ng tr·∫Øng
        - KH√îNG th√™m b·∫•t k·ª≥ ch√∫ th√≠ch hay n·ªôi dung n√†o kh√°c
        - KH√îNG ƒë√°nh s·ªë th·ª© t·ª± c√¢u
        - KH√îNG th√™m k√Ω t·ª± ƒë·∫∑c bi·ªát hay ƒë·ªãnh d·∫°ng kh√°c

        VƒÉn b·∫£n c·∫ßn t√≥m t·∫Øt:
        {article_text}

        Tr·∫£ v·ªÅ k·∫øt qu·∫£ theo ƒë√∫ng ƒë·ªãnh d·∫°ng y√™u c·∫ßu.
        """

        try:
            response = self.abstractive_model.generate_content(prompt)
            if not getattr(response, "text", None):
                print("‚ö†Ô∏è Warning: Empty response from AI model.")
                return ""

            # Clean up the response to match required format
            summary = response.text.strip()
            
            # Use regex to extract and format sentences
            import re
            sentences = re.findall(r'\[(.*?)\]', summary)
            formatted_response = ". ".join(f"[{sentence.strip()}]" for sentence in sentences)
            
            return formatted_response

        except Exception as e:
            print(f"‚ùå Error in generative_extractive: {e}")
            return ""
    
    def abstractive_summarize(self, extractive_sentences: list, word_limit: int = 150):
        """
        Perform abstractive summarization based on extractive sentences.

        Args:
            extractive_sentences (list[str]): List of key sentences from extractive summarization.
            word_limit (int): Desired maximum number of words in the final summary.

        Returns:
            str: Abstractive summary generated by Gemini.
            Returns None if there is an error.
        """
        if not extractive_sentences:
            return "No extractive sentences provided for summarization."
    
        prompt = f"""
        B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n t√≥m t·∫Øt vƒÉn b·∫£n. 
        D∆∞·ªõi ƒë√¢y l√† c√°c c√¢u then ch·ªët ƒë∆∞·ª£c tr√≠ch xu·∫•t t·ª´ vƒÉn b·∫£n g·ªëc:

        ---
        {" ".join(f"- {sentence}" for sentence in extractive_sentences)}
        ---

        **Y√™u c·∫ßu:**
        1. T√≥m t·∫Øt n·ªôi dung ch√≠nh m·ªôt c√°ch t·ª± nhi√™n, d·ªÖ hi·ªÉu, c√≥ t√≠nh li√™n k·∫øt.
        2. Kh√¥ng l·∫∑p l·∫°i nguy√™n vƒÉn, h√£y di·ªÖn ƒë·∫°t l·∫°i s√∫c t√≠ch, gi·ªØ nguy√™n √Ω nghƒ©a.
        3. Kh√¥ng c·∫ßn ti√™u ƒë·ªÅ, d·∫•u g·∫°ch ƒë·∫ßu d√≤ng, emoji, ho·∫∑c b·∫•t k·ª≥ tag n√†o.
        4. Ch·ªâ xu·∫•t ra m·ªôt ƒëo·∫°n vƒÉn duy nh·∫•t, kh√¥ng k√®m th√™m ph·∫ßn m·ªü ƒë·∫ßu, ti√™u ƒë·ªÅ hay k·∫øt lu·∫≠n.
        5. Gi·ªõi h·∫°n ƒë·ªô d√†i t·ªëi ƒëa kho·∫£ng {word_limit} t·ª´.

        **V√≠ d·ª•:**

        _C√¢u then ch·ªët:_
        - Ch·ªß ƒë·ªÅ A c√≥ t√°c ƒë·ªông ƒë√°ng k·ªÉ ƒë·∫øn lƒ©nh v·ª±c B.
        - Nhi·ªÅu chuy√™n gia cho r·∫±ng xu h∆∞·ªõng C s·∫Ω ti·∫øp t·ª•c ph√°t tri·ªÉn.
        - M·ªôt s·ªë y·∫øu t·ªë b√™n ngo√†i nh∆∞ D c≈©ng ·∫£nh h∆∞·ªüng ƒë·∫øn t√¨nh h√¨nh chung.

        _T√≥m t·∫Øt m·∫´u:_
        Ch·ªß ƒë·ªÅ A ƒëang ·∫£nh h∆∞·ªüng m·∫°nh ƒë·∫øn lƒ©nh v·ª±c B, ƒë·ªìng th·ªùi xu h∆∞·ªõng C ƒë∆∞·ª£c k·ª≥ v·ªçng s·∫Ω ti·∫øp t·ª•c ph√°t tri·ªÉn trong th·ªùi gian t·ªõi. C√°c y·∫øu t·ªë b√™n ngo√†i nh∆∞ D c≈©ng g√≥p ph·∫ßn ƒë·ªãnh h√¨nh b·ªëi c·∫£nh hi·ªán t·∫°i.

        ---

        B√¢y gi·ªù, h√£y √°p d·ª•ng c√°ch l√†m t∆∞∆°ng t·ª± v·ªõi c√°c c√¢u then ch·ªët sau v√† ch·ªâ tr·∫£ v·ªÅ ƒëo·∫°n t√≥m t·∫Øt:
        """
        try:
            response = self.abstractive_model.generate_content(prompt)
            summary = response.text.strip()

            return summary

        except Exception as e:
            print(f"Error during abstractive summarization: {e}")
            return None

    def summarize(self, text, num_sentences = 5): # Doing sumamrization on 1 article
        sentences = self.pre_processing(text)
        if not sentences:
            print("No content after preprocessing.")
            return "", []
        extractive_output = self.extractive_summary(sentences, num_sentences=num_sentences)
        summary = self.abstractive_summarize(extractive_output)
        return summary
    
    def process_article(self, article):
        """Helper function to extract key sentences from a single article."""
        sentences = self.pre_processing(article['main_text'])
        if not sentences:
            return None
        
        article_sum = {
            "title": article["title"],
            "url": article["url"],
            "authors": article["author"],
            "date_publish": article["date_publish"],
            "extractive_sum": self.extractive_summary(sentences)
        }
        return article_sum  # Return structured dictionary (not string)

    def multi_source_extractive(self, articles, max_workers=4): 
        """Extract key sentences from multiple articles using parallel processing."""

        if not articles:
            return []

        results = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {executor.submit(self.process_article, article): article for article in articles}
            for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc="Extracting Sentences"):
                try:
                    result = future.result()
                    if result:
                        results.append(result)
                except Exception as e:
                    print(f"Error processing article: {e}")
        
        return results

    def gen_multi_source_extractive(self, articles, max_sentences_per_source=10):
        """
        Generate extractive summaries from multiple articles with source identification.
        
        Args:
            articles (list[dict]): List of articles with main_text field
            max_sentences_per_source (int): Maximum sentences to extract per article
            
        Returns:
            list[str]: List of extractive sentences with source identifiers
        """
        if not articles:
            return []

        extractive_results = []
        
        # Process each article with source identification
        for idx, article in enumerate(articles, 1):
            try:
                # Preprocess and extract sentences
                sentences = (article['main_text'])
                if sentences:
                    extracted_sentences = self.generative_extractive(sentences)
                    
                    # Add source identifier
                    extractive_results.append(f"Title : {article['title']}")
                    extractive_results.append(f"url : {article['url']}")
                    print(f"extracted_sentences: {extracted_sentences}")
                    extractive_results.append(extracted_sentences)
                    
            except Exception as e:
                print(f"Error processing article {idx}: {e}")
                continue

        return extractive_results



    def multi_article_synthesis(self, articles, word_limit=200):
        """
        Generate a synthesized summary from multiple articles.

        Args:
            articles (list[dict]): A list of articles, each represented as a dictionary with field:
                'title', 'main_text', 'url', authors, date_publish 
            with title, url -> from Redis & url -> from scrapper
            num_sentences (int): Number of key sentences to extract per article.
            word_limit (int): Maximum word limit for the final synthesized summary.

        Returns:
            str: A cohesive abstractive summary of all input articles.
        """
        if not articles:
            return "No articles provided for synthesis."

        print(f"Here are the article's lists: {articles}")
        all_extractive_sentences = self.multi_source_extractive(articles)
        # Check if None or empty
        if not all_extractive_sentences:  # This handles both None and empty list cases
            print("No extractive sentences found, falling back to generative extraction...")
            all_extractive_sentences = self.gen_multi_source_extractive(articles)
            if not all_extractive_sentences:  # Double check the fallback result
                return "Could not generate summary: no valid content found in articles."
    
            print(f"Here are all extractive sentences {all_extractive_sentences}")
            """
            list_of_main_text = []
            for all article in article:
                text = "Ngu·ªìn " + article_order + article["main_text"]
                list_of_main_text.push[text]
            
            prompt = "Summary command + list_of_main_text (defined which source order & value) + listy format"

            -> LLM (strictly formated)
            Post-processing => into list ['Ngu·ªìn 1:', "extrc 1", "extrct_k", "Ngu·ªìn 2:", "extrc 1", ... "Ngu·ªìn n: ", "extrc 1", ...] 
            # for all the source -> take 10 extract sentence
            
            """

        # Step 2: Generate a cohesive abstractive summary in a single API call
        prompt = f"""
        B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n t√≥m t·∫Øt n·ªôi dung t·ª´ nhi·ªÅu b√†i b√°o.
        D∆∞·ªõi ƒë√¢y l√† c√°c c√¢u then ch·ªët ƒë∆∞·ª£c tr√≠ch xu·∫•t t·ª´ nhi·ªÅu b√†i vi·∫øt kh√°c nhau - c√πng v·ªõi Meta-Data c·ªßa ch√∫ng:
        [D·ªØ li·ªáu ƒë·∫ßu v√†o]
        ---
         {" ".join(f"- {sentence}" for sentence in all_extractive_sentences)}
        ---

                
        **Y√™u c·∫ßu:**  
        1. H·ª£p nh·∫•t n·ªôi dung t·ª´ nhi·ªÅu b√†i b√°o th√†nh m·ªôt ƒëo·∫°n t√≥m t·∫Øt m·∫°ch l·∫°c.  
        2. Tr√¨nh b√†y s√∫c t√≠ch, d·ªÖ hi·ªÉu, gi·ªØ nguy√™n √Ω nghƒ©a ch√≠nh.  
        3. Kh√¥ng l·∫∑p l·∫°i nguy√™n vƒÉn, di·ªÖn ƒë·∫°t t·ª± nhi√™n.  
        4. Gi·ªõi h·∫°n ƒë·ªô d√†i t·ªëi ƒëa kho·∫£ng {word_limit} t·ª´.  
        5. Kh√¥ng ƒë∆∞·ª£c th√™m b·∫•t c·ª© ph·∫£n h·ªìi hay bi·ªÉu c·∫£m n√†o d∆∞ th·ª´a, ch·ªâ ƒë∆∞·ª£c c√≥ t√≥m t·∫Øt.    
        6. Ch·ªâ t√≥m t·∫Øt, kh√¥ng th√™m √Ω ki·∫øn ch·ªß quan hay b√¨nh lu·∫≠n.  
        7. V·ªõi m·ªói block trong key "reference" 
                - Key l√† s·ªë th·ª© t·ª± b√†i b√°o (1, 2, 3...)
                - M·ªói b√†i c√≥ 2 tr∆∞·ªùng: "title" v√† "url"
        8. H√£y tr√≠ch xu·∫•t t·ª´ [D·ªØ li·ªáu ƒë·∫ßu v√†o] nh∆∞ "title" v√† "url" ƒë·ªÉ tr·∫£ ra k·∫øt qu·∫£
        9. Kh√¥ng ƒë∆∞·ª£c t·∫°o ra url v√† title ·∫£o t·ª´ [D·ªØ li·ªáu ƒë·∫ßu v√†o]
        10. ƒê·ªëi v·ªõi tr∆∞·ªùng "synthesis_paragraph" ph·∫£i ƒë·ªÅ c·∫≠p ƒë·∫øn ngu·ªìn c·ªßa b·∫±ng ch·ª©ng v√† [s·ªë th·ª© t·ª± c·ªßa ngu·ªìn trong "reference" t∆∞∆°ng ·ª©ng]
        **V√≠ d·ª• ph√π h·ª£p c·ªßa ƒë·ªãnh d·∫°ng "key": "value" c·ªßa json, n·∫øu c√≥ d·∫•u ngo·∫∑c k√©p (") trong n·ªôi dung c·ªßa value h√£y ƒë·ªïi th√†nh d·∫•u ngo·∫∑c ƒë∆°n (') ƒë·ªÉ ƒë√∫ng ƒë·ªãnh d·∫°ng json, c·∫•m d√πng d·∫•u ngo·∫∑c k√©p (") khi vi·∫øt ph·∫ßn value:**

        {{
            "synthesis_paragraph": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. [1] Curabitur feugiat ex at quam malesuada sagittis. Aliquam euismod eros tempor magna iaculis placerat. Nam vel faucibus nisl, et convallis nibh. Etiam tempor pulvinar scelerisque. Integer non felis quis risus varius congue nec et nisl [2]. Donec pretium sem eget luctus iaculis. Vestibulum eget condimentum lorem, vitae elementum dui. Vestibulum gravida a magna id imperdiet. Proin lacinia urna a volutpat convallis.",
            "reference": {{
                "1": {{
                    "title": "M·∫•t ng·ªß d·ªÖ g√¢y ch·∫øt ng∆∞·ªùi - B√°o VnExpress S·ª©c kh·ªèe",
                    "url": "https://vnexpress.net/mat-ngu-de-gay-chet-nguoi-3779066.html"
                }},
                "2": {{
                    "title": "6 th√≥i quen 'ch·∫øt ng∆∞·ªùi' d·ªÖ x·∫£y ra ·ªü nh√† v·ªá sinh - B√°o VnExpress ƒê·ªùi s·ªëng",
                    "url": "https://vnexpress.net/6-thoi-quen-chet-nguoi-de-xay-ra-o-nha-ve-sinh-4395632.html"
                }}
                "3": {{
                    "title": "H√†ng trƒÉm ng∆∞·ªùi v·∫° v·∫≠t sau v·ª• ch√°y chung c∆∞ l√†m 13 ng∆∞·ªùi ch·∫øt - B√°o VnExpress",
                    "url": "https://vnexpress.net/6-thoi-quen-chet-nguoi-de-xay-ra-o-nha-ve-sinh-4395632.html"
                }}
            }}
        }}

        **H·∫æT V√ç D·ª§, kh√¥ng ƒë·ªÉ n·ªôi dung v√≠ d·ª• ·∫£nh h∆∞·ªüng ƒë·∫øn ƒë√°nh gi√°**

        H√£y vi·∫øt ƒëo·∫°n t√≥m t·∫Øt ch√≠nh x√°c v√† kh√°ch quan:
        """

        try:
            response = self.abstractive_model.generate_content(prompt)
            return response.text.strip()

        except Exception as e:
            print(f"Error during multi-article synthesis: {e}")
            return None

    def question_and_answer(self, query, evidence=None):
        """
        Answer a question based on either provided evidence or by searching online.
        If evidence is provided, skip the search. Otherwise, perform the search.
        """

        # === Step 1: Understand the question ===
        reasoning = self.qa_utility.understanding_the_question(query)

        # === Step 2: Determine evidence source ===
        all_evidences = []

        if evidence:  # Evidence is provided by user
            for ev in evidence:
                all_evidences.append(
                    f"Source: {ev['title']}\n"
                    f"Author: {ev['author']}\n"
                    f"URL: {ev['url']}\n"
                    f"Snippet: {ev['main_text']}\n"
                )
        else:  # No evidence ‚Üí Perform search
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(self.qa_utility.search_web_fast, query=query)
                while not future.done():
                    time.sleep(0.1)
                search_results = future.result()
                for result in search_results:
                    all_evidences.append(
                        f"Source: {result['title']}\n"
                        f"Author: {result['author']}\n"
                        f"URL: {result['url']}\n"
                        f"Snippet: {result['main_text']}\n"
                    )

        # === Step 3: Synthesize answer ===
        answer = self.qa_utility.synthesize_and_summarize(query=query, reasoning=reasoning, evidence=all_evidences)

        return answer, all_evidences, query
        
    
